because the competition still going on
Therefore, please check it out the link, which contains the full code on Kaggle
[https://www.kaggle.com/code/peggybyby/finetune-0807](https://www.kaggle.com/code/peggybyby/notebook9cb8d0bdf9/edit)



Large Language Models show impressive zero‑shot ability, but fine‑tuning them for a specific downstream task can squeeze out crucial extra performance.
This competition explores parameter efficient training techniques 
(LoRA, QLoRA, P‑Tuning‑v2, etc.) to achieve state‑of‑the‑art results without breaking the bank.
